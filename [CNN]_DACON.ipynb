{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c76e55-3c6f-47b5-83ec-10da904f5fbf",
   "metadata": {},
   "source": [
    "# 📸 [DACON - CNN](https://dacon.io/codeshare/4537)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e87b9-3f59-4658-b264-70947b3e89d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212b14c-8f98-42dd-b82f-ef038b21b7b6",
   "metadata": {},
   "source": [
    "- 영상처리에 탁원한 성능을 자랑하는 **CNN**의 원리를 알아보고, MNIST에 적용해보도록 한다.\n",
    "\n",
    "- 다음으로, CNN을 이용한 **ResNet** 모델로 좀 더 복잡한 컬러 이미지를 다뤄보도록 한다.<br><br>\n",
    "\n",
    "- 현재 과정에서는 CNN 원리에 대해서 간단하게 알아보고, 자세한 모델 Architecture은 추후에 다뤄보도록 한다.\n",
    "\n",
    "- 컴퓨터에서 보는 모든 이미지는 픽셀값들을 가로, 세로로 늘어놓은 행렬로 표현할 수 있다.\n",
    "\n",
    "- **`컨볼루션`** 은 계층적으로 이미지를 인식할 수 있도록 단계마다 이미지의 특징을 추출해주는 것을 의미한다.\n",
    "\n",
    "- **CNN**은 이미지를 추출하는 필터로 Convolution Neural Network, **즉 컨볼루션을 하는 인공 신경망이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ff700-e801-45b3-b6e1-bf8dbd837bd1",
   "metadata": {},
   "source": [
    "- CNN 모델은 일반적으로 **`Convolution Layer`**, **`Pooling Layer`**, 특징들을 모아 최종 분류하는 일반적인 인공신경망 계층으로 구성된다.\n",
    "\n",
    "- 컨볼루션을 거쳐 만들어진 새로운 이미지는 **`특징 맵(Feature Map)`** 이라고도 불린다.\n",
    "\n",
    "- 컨볼루션 계층마다 여러 특징 맵들이 만들어지며, 다음 단계인 **풀링(Pooling)** 계층으로 넘어가게 된다.\n",
    "\n",
    "- 컨볼루션 계층과 폴링 계층을 여러 겹 쌓아, 각 단계에서 만들어진 특징 맵을 관찰하면 CNN 모델이 이미지를 계층적으로 인식하는 것을 볼 수 있다.\n",
    "\n",
    "- **특징 맵의 크기가 크면 학습이 어렵고, 과적합의 위험이 증가한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1110db8-c158-4156-8ff5-8301ca577a18",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\">01. CNN 모델 구현하기</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d256826-1a00-4c24-bce7-8f8aacc6d4a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14d5a7-0280-4b41-933b-7ba192dfeb43",
   "metadata": {},
   "source": [
    "- 여러 CNN 모델은 **Convolution, Pooling, Dropout**, 그리고 **일반적인 신경망 계층**의 조합으로 이루어진다.\n",
    "\n",
    "- **Convolution → Pooling → Convolution → Dropout → Pooling → Flatten → Fully Connected → Dropout → Fully Connected** 예제를 구현해보도록 한다.\n",
    "\n",
    "- 일반 인공신경망을 CNN 계층으로 대체하면 되기 때문에 전체적 구현은 DNN 신경망 구현법과 매우 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f46e2960-94a6-4c87-986b-da92f40113f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version : 1.10.2\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "print(\"PyTorch Version :\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29066fdc-1ac7-418b-b8d5-29df894d6514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9071584b10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d17c9aa8-88f5-4eb2-ae80-b7d0e481a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device : cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if USE_CUDA else \"cpu\"\n",
    "\n",
    "print(\"Using Device :\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610e673-b8b3-40a3-8da3-56cb941b3357",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">1. Hyperparameters</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c9800-30e5-410b-ba2d-17f3cde5f57b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e20ccf26-6e87-4bcd-89f9-a80695b8d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에폭과 배치크기를 정해주도록 한다.\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418f416-9b4c-49cd-85a0-0fdf4340b894",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\"> 2. Data Load</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2e3ad-eb38-4c19-b920-2db5ce55647a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20df88bc-6f55-4a58-84f0-fb01ddcd01f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of First Mini Batch in MNIST train : torch.Size([64, 1, 28, 28]) \n",
      "\n",
      "Shape of First Mini Batch Target in MNIST train : torch.Size([64]) \n",
      "\n",
      "Shape of First Mini Batch in MNIST test : torch.Size([64, 1, 28, 28]) \n",
      "\n",
      "Shape of First Mini Batch Target in MNIST test : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터 불러오기\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    \n",
    "    # 'MNIST()' 함수 적용\n",
    "    datasets.MNIST(\n",
    "                   root = \"./PyTorch로 시작하는 딥러닝 입문/MNIST data/\",\n",
    "                   train = True,\n",
    "                   download = False,\n",
    "                   transform = transforms.Compose([\n",
    "                       \n",
    "                       # PyTorch 텐서화와 정규화 수행\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True)\n",
    "\n",
    "print(\"Shape of First Mini Batch in MNIST train :\", list(train_loader)[0][0].size(), \"\\n\")\n",
    "print(\"Shape of First Mini Batch Target in MNIST train :\", list(train_loader)[0][1].size(), \"\\n\")\n",
    "\n",
    "# 테스트 데이터 불러오기\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "\n",
    "    datasets.MNIST(\n",
    "                   root = \"./PyTorch로 시작하는 딥러닝 입문/MNIST data/\",\n",
    "                   train = False,\n",
    "                   download = False,\n",
    "                   transform = transforms.Compose([\n",
    "                       \n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True)\n",
    "\n",
    "print(\"Shape of First Mini Batch in MNIST test :\", list(test_loader)[0][0].size(), \"\\n\")\n",
    "print(\"Shape of First Mini Batch Target in MNIST test :\", list(test_loader)[0][1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be7773-99e4-4fad-9363-6e5a588bf3f5",
   "metadata": {},
   "source": [
    "- MNIST 데이터는 각 이미지가 28 $\\times$ 28 픽셀로 이루어져 있음을 확인할 수 있으며, 위에서 지정한 Batch Size만큼 DataLoader가 생성되었다.\n",
    "\n",
    "- 위에서 사용된 `transforms.Compose()` 함수는 여러 transforms의 함수들을 구성해주며, 리스트 객체를 입력으로 넣어주면 된다.\n",
    "\n",
    "- 사용된 전처리는 **PyTorch 텐서화**와 **정규화**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784a89b-3715-4884-9335-910df7d0c109",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">3. CNN Model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b923b-6a2f-4660-ae23-4f619e1f68b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce3dfdb4-5e31-46e7-b725-8a28b322a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 클래스 생성\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #-----------------\n",
    "        # 첫번째 컨볼루션 계층\n",
    "        #-----------------\n",
    "        \n",
    "        # 생성한 모델의 커널 크기는 '5 x 5' 이다. 'kernel_size' 매개변수에 숫자를 지정하면 정사각형으로 간주한다.\n",
    "        # 첫번째 컨볼루션 계층을 통해 '10개의 특징맵을 생성한다.'\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 10, kernel_size = 5)\n",
    "        \n",
    "        #-----------------\n",
    "        # 두번째 컨볼루션 계층\n",
    "        #-----------------\n",
    "        \n",
    "        # 두번째 컨볼루션 계층에서는 10개의 특징맵을 받아 20개의 특징맵을 반환하도록 한다.\n",
    "        self.conv2 = nn.Conv2d(in_channels = 10, out_channels = 20, kernel_size = 5)\n",
    "        \n",
    "        #-----------------\n",
    "        # 드롭아웃 계층\n",
    "        #-----------------\n",
    "        \n",
    "        # 컨볼루션 결과 출력값에는 드롭아웃을 해주도록 한다.\n",
    "        # 'p' 매개변수에 따로 지정한 값이 없다면, 0.5의 비율만큼 드롭아웃을 수행한다.\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        \n",
    "        #-----------------\n",
    "        # Fully Connected Layer\n",
    "        #-----------------\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        \n",
    "        # 마지막 출력 결과는 분류할 클래스 개수인 10으로 출력을 설정해주어야 한다.\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 입력 받은 값이 첫번째 컨볼루션 계층을 거치고 'F.max_pool2d()' 함수를 거친다.\n",
    "        # Convolution → Pooling 과정을 거친 후, ReLU 활성화 함수를 거친다.\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 2))\n",
    "        \n",
    "        # 두번째 컨볼루션 계층은 다음과 같은 순서로 진행된다.\n",
    "        # Convolution → Dropout → Pooling → Activation Function\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), kernel_size = 2))\n",
    "        \n",
    "        # 'Fully Connected Layer'에 적용하기 전에 특징맵이 된 x를 1차원으로 Flatten 해주도록 한다.\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c722bec-0f0e-4780-8129-6abde4ec5d0e",
   "metadata": {},
   "source": [
    "- 위에서 **CNN Architecture**를 아래와 같이 구성하였다.\n",
    "\n",
    "- **`Convolution → Pooling → Convolution → Dropout → Pooling → Flatten → Fully Connected → Dropout → Fully Connected`**\n",
    "\n",
    "- 바로 위 `Net` 클래스의 코드를 하나씩 뜯어보면서 살펴보도록 한다. (텐서의 형태 변화를 보고자 하는 것이기 때문에 활성화 함수는 생략하도록 한다.)\n",
    "\n",
    "- 우선 `train_loader`에 저장되어 있는 형태와 똑같은 텐서를 생성해주도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b852298-58cd-471d-aaaa-552a89b5792c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sample Tensor : torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 임의의 텐서 생성\n",
    "tensor = torch.randn(64, 1, 28, 28)\n",
    "print(\"Shape of Sample Tensor :\", tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a9012-d5b8-403c-9aa3-ac45860df4d3",
   "metadata": {},
   "source": [
    "1️⃣ **`Convolution`**\n",
    "\n",
    "- 첫번째 Convolution 계층을 생성하고 적용한 후, 어떻게 변화하는지 살펴보도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9f3a3cc-6368-4edb-864b-3ef5811ca84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Applied First Convolution Layer : torch.Size([64, 10, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 Convolution 계층\n",
    "conv1 = nn.Conv2d(in_channels = 1, out_channels = 10, kernel_size = 5)\n",
    "\n",
    "# 첫번째 Convolution 계층을 거친 후 확인\n",
    "x = conv1(tensor)\n",
    "print(\"Shape of Applied First Convolution Layer :\", x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50d48e-9a1e-46fa-a093-7fa0f4d37abf",
   "metadata": {},
   "source": [
    "- `nn.Conv2d()` 함수를 지나면 형태는 $(N, C_{in}, H, W) \\to (N, C_{out}, H_{out}, W_{out})$과 같이 변하게 된다.\n",
    "\n",
    "- 즉 현재 입력값으로 받은 이미지의 channels는 1이고, `out_channels = 10`으로 지정되어 있으므로 10개의 특징맵이 생성된 것을 확인할 수 있다.\n",
    "\n",
    "- 또한, `kernel_size = 5` 이므로 28 x 28 픽셀의 이미지가 24 x 24 픽셀로 줄어들었음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed82934-f72a-49cf-83c2-447cee457508",
   "metadata": {},
   "source": [
    "2️⃣ **`Convolution → Pooling`**\n",
    "\n",
    "- Convolution 계층 다음으로는 Pooling 층을 적용해보도록 한다.\n",
    "\n",
    "- Pooling 층을 적용할 때, `torch.nn.functional.max_pool2d()` 함수 혹은 `torch.nn.MaxPool2d()` 함수를 사용해도 된다.\n",
    "\n",
    "- 지금 과정에서는 `torch.nn.MaxPool2d()` 함수를 사용해보도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "640c851c-9b14-48f7-9c6c-00be94afd59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Applied First Pooling Layer : torch.Size([64, 10, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 Pooling 계층\n",
    "pool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "# 적용 후 확인\n",
    "x = pool1(x)\n",
    "print(\"Shape of Applied First Pooling Layer :\", x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c9dda-8602-49a3-88a8-3faa258e6a99",
   "metadata": {},
   "source": [
    "- 첫번째 Pooling 계층에 `kerner_size = 2` 이므로 24 x 24 픽셀의 이미지가 절반인 12 x 12 픽셀로 줄어들었음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b289c70-ddd2-49b8-83ec-c12b8595e4de",
   "metadata": {},
   "source": [
    "3️⃣ **`Convolution → Pooling → Convolution`**\n",
    "\n",
    "- 이제 두번째 Convolution 계층을 생성해보도록 한다.\n",
    "\n",
    "- 첫번째 Convolution 계층을 통해 10개의 특징맵이 생성되었고, 10개의 특징맵을 입력으로 받아 20개의 특징맵을 반환하는 계층을 생성해보도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4e72bd3-d7c7-4076-a249-3321997726bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Applied Second Convolution Layer : torch.Size([64, 20, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# 두번째 Convolution 계층\n",
    "conv2 = nn.Conv2d(in_channels = 10, out_channels = 20, kernel_size = 5)\n",
    "\n",
    "# 두번째 Convolution 계층 거친 후 확인\n",
    "x = conv2(x)\n",
    "print(\"Shape of Applied Second Convolution Layer :\", x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2a755-eb93-4d88-a825-370537d3c825",
   "metadata": {},
   "source": [
    "- 두번째 Convolution 계층을 지나고 난 후, 다음과 같이 형태가 변형되었다.\n",
    "\n",
    "- `torch.Size([64, 10, 12, 12]) → torch.Size([64, 20, 8, 8])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec4dc0-3abd-4a39-bc54-676303b06c43",
   "metadata": {},
   "source": [
    "4️⃣ **`Convolution → Pooling → Convolution → Dropout → Pooling`**\n",
    "\n",
    "- 두번째 Convolution 계층을 지나고 나면, Dropout과 Pooling 계층을 지나면 된다.\n",
    "\n",
    "- Dropout 계층을 통해서는 형태 변환이 없기 때문에 Pooling 계층까지 한번에 진행하도록 한다.\n",
    "\n",
    "- 두번째 Pooling 계층에서도 마찬가지로 `kernel_size = 2`로 지정해주도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ceb325b1-d864-4175-9911-eddb8503ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Applied First Dropout Layer : torch.Size([64, 20, 8, 8]) \n",
      "\n",
      "Shape of Applied Second Pooling Layer : torch.Size([64, 20, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# 1. Dropout 계층 생성\n",
    "drop1 = nn.Dropout2d()\n",
    "\n",
    "# Dropout 계층 적용\n",
    "x = drop1(x)\n",
    "print(\"Shape of Applied First Dropout Layer :\", x.size(), \"\\n\")\n",
    "\n",
    "# 2. Pooling 계층 생성\n",
    "pool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "# Pooling 계층 적용\n",
    "x = pool2(x)\n",
    "print(\"Shape of Applied Second Pooling Layer :\", x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216fa77-5002-4ad8-ba85-69d1dab2b7f9",
   "metadata": {},
   "source": [
    "- Dropout 계층을 통해서는 형태 변환이 없고, Pooling 계층을 지나고 난 후 형태가 변했음을 확인할 수 있다.\n",
    "\n",
    "- `kernel_size = 2` 이므로 8 x 8 픽셀의 이미지가 절반인 4 x 4 픽셀로 줄어들었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f8793b-f169-4055-80b4-d15b186ebba1",
   "metadata": {},
   "source": [
    "5️⃣ **`Convolution → Pooling → Convolution → Dropout → Pooling → Flatten`**\n",
    "\n",
    "- Convolution, Pooling, Dropout을 통해 **Feature Extraction** 과정을 진행했으므로 그 다음으로는 **Fully Connected Layer**를 통해 이미지를 분류해주면 된다.\n",
    "\n",
    "- Fully Connected Layer에 입력값을 넣어주기 전에 이미지 데이터이므로 입력값을 1차원 벡터로 변경해주는 작업이 필요하다.\n",
    "\n",
    "- 지금까지의 과정을 거쳐 20개의 특성맵과 4 x 4 픽셀의 형태를 가지기 때문에, $20 \\times 4 \\times 4 = 320$ 이다.\n",
    "\n",
    "- 즉, 320개의 특성을 가지는 1차원 벡터를 생성해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "303025e3-6109-4840-96b4-e785d6637a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Flatten tensor : torch.Size([64, 320])\n"
     ]
    }
   ],
   "source": [
    "# Flatten 과정 진행\n",
    "x = x.view(-1, 320)\n",
    "print(\"Shape of Flatten tensor :\", x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252b29b-3dd6-48c4-a349-5ab4a40d5f70",
   "metadata": {},
   "source": [
    "6️⃣ **`Convolution → Pooling → Convolution → Dropout → Pooling → Flatten → Fully Connected`**\n",
    "\n",
    "- 2차원 이미지 데이터의 Flatten 과정을 거쳐 1차원 벡터로 변경해주었으므로, `torch.nn.Linear()` 함수를 통해 Fully Connected Layer를 생성하면 된다.\n",
    "\n",
    "- 320개의 입력값을 받아서 50개를 출력값으로 반환하는 계층을 생성하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7531ce86-b768-41a6-96cd-7458e3b195a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Applied First Fully Connected Layer : torch.Size([64, 50])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 Fully Connected 계층 생성\n",
    "fully1 = nn.Linear(320, 50)\n",
    "\n",
    "# Fully Connected 계층 적용\n",
    "x = fully1(x)\n",
    "print(\"Shape of Applied First Fully Connected Layer :\", x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb93f40-af38-49f6-9bb8-ce11c42968d8",
   "metadata": {},
   "source": [
    "7️⃣ **`Convolution → Pooling → Convolution → Dropout → Pooling → Flatten → Fully Connected → Dropout → Fully Connected`**\n",
    "\n",
    "- 이제 마지막으로 Dropout과 Fully Connected 계층을 생성해주고 적용해주도록 한다.\n",
    "\n",
    "- 또한, 마지막 Fully Connected 계층에는 출력값이 해당 데이터 클래스의 개수이어야 한다.\n",
    "\n",
    "- 현재 과정에서는 10이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5eeebac-e310-4dbf-b4c9-ec0431150a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Applied Second Dropout Layer : torch.Size([64, 50]) \n",
      "\n",
      "Shape of Applied Second Fully Connected Layer : torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# 1. Dropout 계층 생성\n",
    "drop2 = nn.Dropout()\n",
    "\n",
    "# Dropout 계층 적용\n",
    "x = drop2(x)\n",
    "print(\"Shape of Applied Second Dropout Layer :\", x.size(), \"\\n\")\n",
    "\n",
    "# 2. Fully Connected 계층 생성\n",
    "fully2 = nn.Linear(50, 10)\n",
    "\n",
    "# Fully Connected 계층 적용\n",
    "x = fully2(x)\n",
    "print(\"Shape of Applied Second Fully Connected Layer :\", x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff35d-2c71-4c3e-bf00-d70a7d7db794",
   "metadata": {},
   "source": [
    "- 이로써 7번의 과정을 통해 위에서 정의한 CNN 모델 클래스 객체를 살펴보았다.\n",
    "\n",
    "- 7번 과정에서 주의할 점은 Dropout을 진행할 때 **`torch.nn.Dropout()`** 함수를 사용하였고, 4번 과정에서는 **`torch.nn.Dropout2d()`** 함수를 사용하였다는 것이다.\n",
    "\n",
    "- 4번 과정에서는 2차원의 이미지 데이터이기 때문에 `torch.nn.Dropout2d()`를 사용하였고, 7번 과정에서는 Flatten을 진행해준 1차원 데이터이기 때문에 `torch.nn.Dropout()`를 사용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63606d0e-e2a2-4ae0-9ca4-75fc53fe1faa",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">4. Model Train</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b312d-33a9-4721-b0bf-adcb1db394f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79ca7714-4b81-4bf4-9a5d-a071f5f69ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 객체 생성\n",
    "model = Net().to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# Optimizer 생성\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "72c86f4d-035b-4f24-af2b-d6175ba16d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 훈련을 진행할 수 있는 함수 생성\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    \n",
    "    # 모델을 학습 모드로 전환\n",
    "    model.train()\n",
    "    \n",
    "    # 반복문을 통해 학습 진행\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # 입력된 모델을 통해 데이터 학습\n",
    "        output = model(data)\n",
    "        \n",
    "        # Gradients 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 분류 문제이기 때문에 'torch.nn.functional.cross_entropy()' 함수를 사용한다.\n",
    "        # 'torch.nn.functional.cross_entropy()' 함수는 소프트맥스 함수까지 포함하고 있음을 기억해야 한다.\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # Gradient Descent 수행\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print(\"Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(epoch,\n",
    "                                                                              EPOCHS,\n",
    "                                                                              batch_idx * len(data),\n",
    "                                                                              len(train_loader.dataset),\n",
    "                                                                              100. * batch_idx / len(train_loader),\n",
    "                                                                              loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f312d97d-f3d3-4cde-ad98-36c253cc0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련된 모델로 성능을 확인할 수 있는 함수 생성\n",
    "def evaluate(model, test_loader):\n",
    "    \n",
    "    # 훈련된 모델을 평가 모드로 전환\n",
    "    model.eval()\n",
    "    \n",
    "    # 미니 배치를 다 수행한 후, 결과를 확인하기 위해 초기값 지정\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # 가중치 변환이 일어나지 않도록 지정\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            \n",
    "            # 학습된 모델을 통해 예측값 생성\n",
    "            output = model(data)\n",
    "            \n",
    "            # 배치의 오차를 합하기\n",
    "            test_loss += F.cross_entropy(output, target, reduction = \"sum\").item()\n",
    "            \n",
    "            # 소프트맥스 함수를 통해 10개 클래스의 확률이 반환되며, 가장 높은 값을 가진 인덱스가 예측값이다.\n",
    "            pred = output.max(1, keepdim = True)[1]\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    # 위 과정을 통해 모든 배치에 대한 평가가 완료되었으며, 손실값을 모두 더해주었기 때문에 테스트 데이터의 개수(10,000)로 나눠주면 된다.\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a07a2a-b23f-4357-830f-3a06858f0bba",
   "metadata": {},
   "source": [
    "- `evaluate` 함수를 통해 훈련된 모델로 테스트 데이터를 예측하면서 성능을 확인할 수 있다.\n",
    "\n",
    "- `evaluate` 함수 내부의 코드를 자세히 살펴보도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea72be5-e168-40d6-b967-ab8c418f4757",
   "metadata": {},
   "source": [
    "```python\n",
    "1️⃣ test_loss += F.cross_entropy(output, target, reduction = \"sum\").item()\n",
    "```\n",
    "\n",
    "- 위의 `torch.nn.functional.cross_entropy()` 함수는 Cross Entropy를 구해주며, `reduction = \"sum\"`은 구해진 손실값을 모두 더하라는 의미이다.\n",
    "\n",
    "- **그렇다면, 미니 배치를 학습하면서 손실값을 더해주는 이유는 무엇일까?**\n",
    "\n",
    "현재 과정에서 배치 크기는 64이며, 64개 데이터에 대한 손실값을 $64Loss$ 라고 가정해보도록 하자.\n",
    "\n",
    "우리가 구하고자 하는 테스트 데이터의 손실값은 10,000개 데이터를 모두 예측한 후의 Cross Entropy 값이다.\n",
    "\n",
    "즉, 수식으로 보면 아래와 같다. \n",
    "\n",
    "$$Cost(W) = \\frac{10,000Loss(=\\ 64Loss + 64Loss + ... + 64Loss)}{10,000}$$\n",
    "\n",
    "만약, 위의 `reduction` 매개변수에 `mean` 인자를 넣어주면 아래와 같이 된다.\n",
    "\n",
    "$$\\frac{64Loss}{64} +\\ \\frac{64Loss}{64} +\\ ... +\\ \\frac{64Loss}{64} \\ne \\frac{64Loss +\\ 64Loss +\\ ... +\\ 64Loss}{10,000}$$\n",
    "\n",
    "따라서, 위와 같은 이유 떄문에 미니 배치를 모두 학습한 손실값을 확인하려면 **`reduction = \"sum\"`** 으로 입력해주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f36ff-e86c-48df-851a-2f6dda92deef",
   "metadata": {},
   "source": [
    "```python\n",
    "2️⃣ pred = output.max(1, keepdim = True)[1]\n",
    "```\n",
    "\n",
    "- 위에서 생성한 CNN 모델을 통해 나온 텐서의 형태는 **`torch.Size([64, 10])`** 이다.\n",
    "\n",
    "- 64개 각각 관측치가 10개 클래스에 속할 확률을 의미하는 것이다.\n",
    "\n",
    "- 첫번째 인자 `1`은 행방향으로 연산을 수행하라는 의미이며, `keepdim = True`는 `output` 텐서와 크기를 똑같이 반환하라는 의미이다.\n",
    "\n",
    "- `keepdim = True`의 경우에는 **torch.Size([64, 1])** 이다.\n",
    "\n",
    "- `keepdim = False`의 경우에는 **torch.Size([64])** 이다.\n",
    "\n",
    "- 아래의 코드를 통해 한번 더 확인하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1789f395-6370-4b57-8cf0-c8ff6ece1635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Output Size : torch.Size([64, 10]) \n",
      "\n",
      "Maximum *keepdim = True* : torch.Size([64, 1]) \n",
      "\n",
      "Maximum *Keepdim = False* : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN Model Output Size :\", x.size(), \"\\n\")\n",
    "\n",
    "# 각 관측치가 10개 클래스에 속할 확률 중, 가장 큰 값 반환\n",
    "print(\"Maximum *keepdim = True* :\", x.max(1, keepdim = True)[1].size(), \"\\n\")\n",
    "print(\"Maximum *Keepdim = False* :\", x.max(1)[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c2065-9c10-4a58-aae3-0f0785684af5",
   "metadata": {},
   "source": [
    "```python\n",
    "3️⃣ correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "```\n",
    "\n",
    "- 우선 가장 내부의 `target.view_as(pred)`의 **`view_as()`** 함수는 인자로 주어진 텐서와 똑같은 형태로 만들라는 것이다.\n",
    "\n",
    "- 현재 `pred` 변수의 형태는 `torch.Size([64, 1])` 이며, `target` 변수의 형태는 `torch.Size([64])` 이다.<br><br>\n",
    "\n",
    "- 다음으로 `tensor.eq()` 메서드는 요소별로 동등 여부를 비교한다. 예시 결과는 해당 [링크](https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq)를 통해 확인한다.\n",
    "\n",
    "- 즉, 예측값과 실제값이 같으면 **True**를 반환하고, 예측값과 실제값이 다르면 **False**를 반환한다.<br><br>\n",
    "\n",
    "- 위의 과정을 마친 후, 해당 텐서는 True 혹은 False로 이루어져 있으며 `tensor.sum()` 메서드를 사용하면 True 개수의 합을 구해준다.\n",
    "\n",
    "- 손실값과 마찬가지로 모두 더해준 후, 모든 미니 배치 학습을 마치고 테스트 데이터의 개수로 나눠주면 정확도가 나온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c302481d-11d1-4ef8-83aa-bf937221b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/40 [0/60000 (0%)]\tLoss: 2.356333\n",
      "Train Epoch: 1/40 [12800/60000 (21%)]\tLoss: 1.287697\n",
      "Train Epoch: 1/40 [25600/60000 (43%)]\tLoss: 0.771791\n",
      "Train Epoch: 1/40 [38400/60000 (64%)]\tLoss: 0.683238\n",
      "Train Epoch: 1/40 [51200/60000 (85%)]\tLoss: 0.381381\n",
      "\n",
      "[1] Test Loss: 0.1933, Accuracy: 94.42%\n",
      "\n",
      "Train Epoch: 2/40 [0/60000 (0%)]\tLoss: 0.473227\n",
      "Train Epoch: 2/40 [12800/60000 (21%)]\tLoss: 0.658656\n",
      "Train Epoch: 2/40 [25600/60000 (43%)]\tLoss: 0.372008\n",
      "Train Epoch: 2/40 [38400/60000 (64%)]\tLoss: 0.301509\n",
      "Train Epoch: 2/40 [51200/60000 (85%)]\tLoss: 0.213536\n",
      "\n",
      "[2] Test Loss: 0.1181, Accuracy: 96.31%\n",
      "\n",
      "Train Epoch: 3/40 [0/60000 (0%)]\tLoss: 0.510756\n",
      "Train Epoch: 3/40 [12800/60000 (21%)]\tLoss: 0.403681\n",
      "Train Epoch: 3/40 [25600/60000 (43%)]\tLoss: 0.301711\n",
      "Train Epoch: 3/40 [38400/60000 (64%)]\tLoss: 0.244048\n",
      "Train Epoch: 3/40 [51200/60000 (85%)]\tLoss: 0.186706\n",
      "\n",
      "[3] Test Loss: 0.0910, Accuracy: 97.09%\n",
      "\n",
      "Train Epoch: 4/40 [0/60000 (0%)]\tLoss: 0.531123\n",
      "Train Epoch: 4/40 [12800/60000 (21%)]\tLoss: 0.172690\n",
      "Train Epoch: 4/40 [25600/60000 (43%)]\tLoss: 0.299941\n",
      "Train Epoch: 4/40 [38400/60000 (64%)]\tLoss: 0.322637\n",
      "Train Epoch: 4/40 [51200/60000 (85%)]\tLoss: 0.402981\n",
      "\n",
      "[4] Test Loss: 0.0786, Accuracy: 97.44%\n",
      "\n",
      "Train Epoch: 5/40 [0/60000 (0%)]\tLoss: 0.180197\n",
      "Train Epoch: 5/40 [12800/60000 (21%)]\tLoss: 0.162394\n",
      "Train Epoch: 5/40 [25600/60000 (43%)]\tLoss: 0.289916\n",
      "Train Epoch: 5/40 [38400/60000 (64%)]\tLoss: 0.244183\n",
      "Train Epoch: 5/40 [51200/60000 (85%)]\tLoss: 0.209680\n",
      "\n",
      "[5] Test Loss: 0.0682, Accuracy: 97.68%\n",
      "\n",
      "Train Epoch: 6/40 [0/60000 (0%)]\tLoss: 0.169115\n",
      "Train Epoch: 6/40 [12800/60000 (21%)]\tLoss: 0.088134\n",
      "Train Epoch: 6/40 [25600/60000 (43%)]\tLoss: 0.203420\n",
      "Train Epoch: 6/40 [38400/60000 (64%)]\tLoss: 0.167483\n",
      "Train Epoch: 6/40 [51200/60000 (85%)]\tLoss: 0.150326\n",
      "\n",
      "[6] Test Loss: 0.0596, Accuracy: 98.01%\n",
      "\n",
      "Train Epoch: 7/40 [0/60000 (0%)]\tLoss: 0.180353\n",
      "Train Epoch: 7/40 [12800/60000 (21%)]\tLoss: 0.114513\n",
      "Train Epoch: 7/40 [25600/60000 (43%)]\tLoss: 0.173021\n",
      "Train Epoch: 7/40 [38400/60000 (64%)]\tLoss: 0.263193\n",
      "Train Epoch: 7/40 [51200/60000 (85%)]\tLoss: 0.078323\n",
      "\n",
      "[7] Test Loss: 0.0548, Accuracy: 98.21%\n",
      "\n",
      "Train Epoch: 8/40 [0/60000 (0%)]\tLoss: 0.223741\n",
      "Train Epoch: 8/40 [12800/60000 (21%)]\tLoss: 0.171136\n",
      "Train Epoch: 8/40 [25600/60000 (43%)]\tLoss: 0.058884\n",
      "Train Epoch: 8/40 [38400/60000 (64%)]\tLoss: 0.206811\n",
      "Train Epoch: 8/40 [51200/60000 (85%)]\tLoss: 0.108467\n",
      "\n",
      "[8] Test Loss: 0.0537, Accuracy: 98.38%\n",
      "\n",
      "Train Epoch: 9/40 [0/60000 (0%)]\tLoss: 0.214662\n",
      "Train Epoch: 9/40 [12800/60000 (21%)]\tLoss: 0.114073\n",
      "Train Epoch: 9/40 [25600/60000 (43%)]\tLoss: 0.221230\n",
      "Train Epoch: 9/40 [38400/60000 (64%)]\tLoss: 0.248038\n",
      "Train Epoch: 9/40 [51200/60000 (85%)]\tLoss: 0.086229\n",
      "\n",
      "[9] Test Loss: 0.0520, Accuracy: 98.27%\n",
      "\n",
      "Train Epoch: 10/40 [0/60000 (0%)]\tLoss: 0.201402\n",
      "Train Epoch: 10/40 [12800/60000 (21%)]\tLoss: 0.237034\n",
      "Train Epoch: 10/40 [25600/60000 (43%)]\tLoss: 0.212221\n",
      "Train Epoch: 10/40 [38400/60000 (64%)]\tLoss: 0.073778\n",
      "Train Epoch: 10/40 [51200/60000 (85%)]\tLoss: 0.064708\n",
      "\n",
      "[10] Test Loss: 0.0481, Accuracy: 98.30%\n",
      "\n",
      "Train Epoch: 11/40 [0/60000 (0%)]\tLoss: 0.239978\n",
      "Train Epoch: 11/40 [12800/60000 (21%)]\tLoss: 0.207450\n",
      "Train Epoch: 11/40 [25600/60000 (43%)]\tLoss: 0.180081\n",
      "Train Epoch: 11/40 [38400/60000 (64%)]\tLoss: 0.103028\n",
      "Train Epoch: 11/40 [51200/60000 (85%)]\tLoss: 0.354638\n",
      "\n",
      "[11] Test Loss: 0.0472, Accuracy: 98.49%\n",
      "\n",
      "Train Epoch: 12/40 [0/60000 (0%)]\tLoss: 0.117946\n",
      "Train Epoch: 12/40 [12800/60000 (21%)]\tLoss: 0.058200\n",
      "Train Epoch: 12/40 [25600/60000 (43%)]\tLoss: 0.165620\n",
      "Train Epoch: 12/40 [38400/60000 (64%)]\tLoss: 0.049911\n",
      "Train Epoch: 12/40 [51200/60000 (85%)]\tLoss: 0.185866\n",
      "\n",
      "[12] Test Loss: 0.0464, Accuracy: 98.51%\n",
      "\n",
      "Train Epoch: 13/40 [0/60000 (0%)]\tLoss: 0.230908\n",
      "Train Epoch: 13/40 [12800/60000 (21%)]\tLoss: 0.063948\n",
      "Train Epoch: 13/40 [25600/60000 (43%)]\tLoss: 0.129459\n",
      "Train Epoch: 13/40 [38400/60000 (64%)]\tLoss: 0.262756\n",
      "Train Epoch: 13/40 [51200/60000 (85%)]\tLoss: 0.126165\n",
      "\n",
      "[13] Test Loss: 0.0441, Accuracy: 98.47%\n",
      "\n",
      "Train Epoch: 14/40 [0/60000 (0%)]\tLoss: 0.141018\n",
      "Train Epoch: 14/40 [12800/60000 (21%)]\tLoss: 0.130706\n",
      "Train Epoch: 14/40 [25600/60000 (43%)]\tLoss: 0.068239\n",
      "Train Epoch: 14/40 [38400/60000 (64%)]\tLoss: 0.146978\n",
      "Train Epoch: 14/40 [51200/60000 (85%)]\tLoss: 0.192883\n",
      "\n",
      "[14] Test Loss: 0.0425, Accuracy: 98.61%\n",
      "\n",
      "Train Epoch: 15/40 [0/60000 (0%)]\tLoss: 0.110806\n",
      "Train Epoch: 15/40 [12800/60000 (21%)]\tLoss: 0.114052\n",
      "Train Epoch: 15/40 [25600/60000 (43%)]\tLoss: 0.091052\n",
      "Train Epoch: 15/40 [38400/60000 (64%)]\tLoss: 0.480706\n",
      "Train Epoch: 15/40 [51200/60000 (85%)]\tLoss: 0.099621\n",
      "\n",
      "[15] Test Loss: 0.0405, Accuracy: 98.72%\n",
      "\n",
      "Train Epoch: 16/40 [0/60000 (0%)]\tLoss: 0.208123\n",
      "Train Epoch: 16/40 [12800/60000 (21%)]\tLoss: 0.070380\n",
      "Train Epoch: 16/40 [25600/60000 (43%)]\tLoss: 0.088214\n",
      "Train Epoch: 16/40 [38400/60000 (64%)]\tLoss: 0.031920\n",
      "Train Epoch: 16/40 [51200/60000 (85%)]\tLoss: 0.038076\n",
      "\n",
      "[16] Test Loss: 0.0373, Accuracy: 98.79%\n",
      "\n",
      "Train Epoch: 17/40 [0/60000 (0%)]\tLoss: 0.192070\n",
      "Train Epoch: 17/40 [12800/60000 (21%)]\tLoss: 0.138461\n",
      "Train Epoch: 17/40 [25600/60000 (43%)]\tLoss: 0.071582\n",
      "Train Epoch: 17/40 [38400/60000 (64%)]\tLoss: 0.159558\n",
      "Train Epoch: 17/40 [51200/60000 (85%)]\tLoss: 0.306357\n",
      "\n",
      "[17] Test Loss: 0.0411, Accuracy: 98.52%\n",
      "\n",
      "Train Epoch: 18/40 [0/60000 (0%)]\tLoss: 0.068767\n",
      "Train Epoch: 18/40 [12800/60000 (21%)]\tLoss: 0.067424\n",
      "Train Epoch: 18/40 [25600/60000 (43%)]\tLoss: 0.265747\n",
      "Train Epoch: 18/40 [38400/60000 (64%)]\tLoss: 0.195892\n",
      "Train Epoch: 18/40 [51200/60000 (85%)]\tLoss: 0.160708\n",
      "\n",
      "[18] Test Loss: 0.0368, Accuracy: 98.81%\n",
      "\n",
      "Train Epoch: 19/40 [0/60000 (0%)]\tLoss: 0.105447\n",
      "Train Epoch: 19/40 [12800/60000 (21%)]\tLoss: 0.195262\n",
      "Train Epoch: 19/40 [25600/60000 (43%)]\tLoss: 0.119242\n",
      "Train Epoch: 19/40 [38400/60000 (64%)]\tLoss: 0.096417\n",
      "Train Epoch: 19/40 [51200/60000 (85%)]\tLoss: 0.027058\n",
      "\n",
      "[19] Test Loss: 0.0389, Accuracy: 98.74%\n",
      "\n",
      "Train Epoch: 20/40 [0/60000 (0%)]\tLoss: 0.119958\n",
      "Train Epoch: 20/40 [12800/60000 (21%)]\tLoss: 0.151485\n",
      "Train Epoch: 20/40 [25600/60000 (43%)]\tLoss: 0.032295\n",
      "Train Epoch: 20/40 [38400/60000 (64%)]\tLoss: 0.083424\n",
      "Train Epoch: 20/40 [51200/60000 (85%)]\tLoss: 0.154333\n",
      "\n",
      "[20] Test Loss: 0.0374, Accuracy: 98.83%\n",
      "\n",
      "Train Epoch: 21/40 [0/60000 (0%)]\tLoss: 0.069744\n",
      "Train Epoch: 21/40 [12800/60000 (21%)]\tLoss: 0.073322\n",
      "Train Epoch: 21/40 [25600/60000 (43%)]\tLoss: 0.366059\n",
      "Train Epoch: 21/40 [38400/60000 (64%)]\tLoss: 0.200044\n",
      "Train Epoch: 21/40 [51200/60000 (85%)]\tLoss: 0.023690\n",
      "\n",
      "[21] Test Loss: 0.0359, Accuracy: 98.85%\n",
      "\n",
      "Train Epoch: 22/40 [0/60000 (0%)]\tLoss: 0.077429\n",
      "Train Epoch: 22/40 [12800/60000 (21%)]\tLoss: 0.090833\n",
      "Train Epoch: 22/40 [25600/60000 (43%)]\tLoss: 0.063065\n",
      "Train Epoch: 22/40 [38400/60000 (64%)]\tLoss: 0.064686\n",
      "Train Epoch: 22/40 [51200/60000 (85%)]\tLoss: 0.055026\n",
      "\n",
      "[22] Test Loss: 0.0341, Accuracy: 98.92%\n",
      "\n",
      "Train Epoch: 23/40 [0/60000 (0%)]\tLoss: 0.124904\n",
      "Train Epoch: 23/40 [12800/60000 (21%)]\tLoss: 0.242069\n",
      "Train Epoch: 23/40 [25600/60000 (43%)]\tLoss: 0.056555\n",
      "Train Epoch: 23/40 [38400/60000 (64%)]\tLoss: 0.052323\n",
      "Train Epoch: 23/40 [51200/60000 (85%)]\tLoss: 0.113622\n",
      "\n",
      "[23] Test Loss: 0.0359, Accuracy: 98.91%\n",
      "\n",
      "Train Epoch: 24/40 [0/60000 (0%)]\tLoss: 0.190739\n",
      "Train Epoch: 24/40 [12800/60000 (21%)]\tLoss: 0.155624\n",
      "Train Epoch: 24/40 [25600/60000 (43%)]\tLoss: 0.111896\n",
      "Train Epoch: 24/40 [38400/60000 (64%)]\tLoss: 0.069772\n",
      "Train Epoch: 24/40 [51200/60000 (85%)]\tLoss: 0.099301\n",
      "\n",
      "[24] Test Loss: 0.0354, Accuracy: 98.83%\n",
      "\n",
      "Train Epoch: 25/40 [0/60000 (0%)]\tLoss: 0.120277\n",
      "Train Epoch: 25/40 [12800/60000 (21%)]\tLoss: 0.154716\n",
      "Train Epoch: 25/40 [25600/60000 (43%)]\tLoss: 0.136415\n",
      "Train Epoch: 25/40 [38400/60000 (64%)]\tLoss: 0.142567\n",
      "Train Epoch: 25/40 [51200/60000 (85%)]\tLoss: 0.084633\n",
      "\n",
      "[25] Test Loss: 0.0369, Accuracy: 98.83%\n",
      "\n",
      "Train Epoch: 26/40 [0/60000 (0%)]\tLoss: 0.062824\n",
      "Train Epoch: 26/40 [12800/60000 (21%)]\tLoss: 0.112801\n",
      "Train Epoch: 26/40 [25600/60000 (43%)]\tLoss: 0.158747\n",
      "Train Epoch: 26/40 [38400/60000 (64%)]\tLoss: 0.073353\n",
      "Train Epoch: 26/40 [51200/60000 (85%)]\tLoss: 0.108965\n",
      "\n",
      "[26] Test Loss: 0.0326, Accuracy: 98.95%\n",
      "\n",
      "Train Epoch: 27/40 [0/60000 (0%)]\tLoss: 0.135109\n",
      "Train Epoch: 27/40 [12800/60000 (21%)]\tLoss: 0.123420\n",
      "Train Epoch: 27/40 [25600/60000 (43%)]\tLoss: 0.077055\n",
      "Train Epoch: 27/40 [38400/60000 (64%)]\tLoss: 0.095014\n",
      "Train Epoch: 27/40 [51200/60000 (85%)]\tLoss: 0.127136\n",
      "\n",
      "[27] Test Loss: 0.0341, Accuracy: 98.94%\n",
      "\n",
      "Train Epoch: 28/40 [0/60000 (0%)]\tLoss: 0.087692\n",
      "Train Epoch: 28/40 [12800/60000 (21%)]\tLoss: 0.049737\n",
      "Train Epoch: 28/40 [25600/60000 (43%)]\tLoss: 0.129803\n",
      "Train Epoch: 28/40 [38400/60000 (64%)]\tLoss: 0.089164\n",
      "Train Epoch: 28/40 [51200/60000 (85%)]\tLoss: 0.132598\n",
      "\n",
      "[28] Test Loss: 0.0337, Accuracy: 98.95%\n",
      "\n",
      "Train Epoch: 29/40 [0/60000 (0%)]\tLoss: 0.062411\n",
      "Train Epoch: 29/40 [12800/60000 (21%)]\tLoss: 0.054877\n",
      "Train Epoch: 29/40 [25600/60000 (43%)]\tLoss: 0.100221\n",
      "Train Epoch: 29/40 [38400/60000 (64%)]\tLoss: 0.043790\n",
      "Train Epoch: 29/40 [51200/60000 (85%)]\tLoss: 0.153187\n",
      "\n",
      "[29] Test Loss: 0.0355, Accuracy: 98.84%\n",
      "\n",
      "Train Epoch: 30/40 [0/60000 (0%)]\tLoss: 0.048094\n",
      "Train Epoch: 30/40 [12800/60000 (21%)]\tLoss: 0.179512\n",
      "Train Epoch: 30/40 [25600/60000 (43%)]\tLoss: 0.028807\n",
      "Train Epoch: 30/40 [38400/60000 (64%)]\tLoss: 0.093438\n",
      "Train Epoch: 30/40 [51200/60000 (85%)]\tLoss: 0.062279\n",
      "\n",
      "[30] Test Loss: 0.0332, Accuracy: 98.96%\n",
      "\n",
      "Train Epoch: 31/40 [0/60000 (0%)]\tLoss: 0.087123\n",
      "Train Epoch: 31/40 [12800/60000 (21%)]\tLoss: 0.061057\n",
      "Train Epoch: 31/40 [25600/60000 (43%)]\tLoss: 0.138490\n",
      "Train Epoch: 31/40 [38400/60000 (64%)]\tLoss: 0.174807\n",
      "Train Epoch: 31/40 [51200/60000 (85%)]\tLoss: 0.064371\n",
      "\n",
      "[31] Test Loss: 0.0340, Accuracy: 98.88%\n",
      "\n",
      "Train Epoch: 32/40 [0/60000 (0%)]\tLoss: 0.062965\n",
      "Train Epoch: 32/40 [12800/60000 (21%)]\tLoss: 0.329680\n",
      "Train Epoch: 32/40 [25600/60000 (43%)]\tLoss: 0.117726\n",
      "Train Epoch: 32/40 [38400/60000 (64%)]\tLoss: 0.176894\n",
      "Train Epoch: 32/40 [51200/60000 (85%)]\tLoss: 0.143228\n",
      "\n",
      "[32] Test Loss: 0.0327, Accuracy: 98.95%\n",
      "\n",
      "Train Epoch: 33/40 [0/60000 (0%)]\tLoss: 0.049707\n",
      "Train Epoch: 33/40 [12800/60000 (21%)]\tLoss: 0.111536\n",
      "Train Epoch: 33/40 [25600/60000 (43%)]\tLoss: 0.219482\n",
      "Train Epoch: 33/40 [38400/60000 (64%)]\tLoss: 0.062088\n",
      "Train Epoch: 33/40 [51200/60000 (85%)]\tLoss: 0.119359\n",
      "\n",
      "[33] Test Loss: 0.0339, Accuracy: 98.88%\n",
      "\n",
      "Train Epoch: 34/40 [0/60000 (0%)]\tLoss: 0.218954\n",
      "Train Epoch: 34/40 [12800/60000 (21%)]\tLoss: 0.136402\n",
      "Train Epoch: 34/40 [25600/60000 (43%)]\tLoss: 0.140007\n",
      "Train Epoch: 34/40 [38400/60000 (64%)]\tLoss: 0.037410\n",
      "Train Epoch: 34/40 [51200/60000 (85%)]\tLoss: 0.247558\n",
      "\n",
      "[34] Test Loss: 0.0330, Accuracy: 98.94%\n",
      "\n",
      "Train Epoch: 35/40 [0/60000 (0%)]\tLoss: 0.095514\n",
      "Train Epoch: 35/40 [12800/60000 (21%)]\tLoss: 0.141288\n",
      "Train Epoch: 35/40 [25600/60000 (43%)]\tLoss: 0.044149\n",
      "Train Epoch: 35/40 [38400/60000 (64%)]\tLoss: 0.110857\n",
      "Train Epoch: 35/40 [51200/60000 (85%)]\tLoss: 0.115708\n",
      "\n",
      "[35] Test Loss: 0.0312, Accuracy: 99.04%\n",
      "\n",
      "Train Epoch: 36/40 [0/60000 (0%)]\tLoss: 0.217767\n",
      "Train Epoch: 36/40 [12800/60000 (21%)]\tLoss: 0.147641\n",
      "Train Epoch: 36/40 [25600/60000 (43%)]\tLoss: 0.278736\n",
      "Train Epoch: 36/40 [38400/60000 (64%)]\tLoss: 0.106778\n",
      "Train Epoch: 36/40 [51200/60000 (85%)]\tLoss: 0.168384\n",
      "\n",
      "[36] Test Loss: 0.0318, Accuracy: 99.01%\n",
      "\n",
      "Train Epoch: 37/40 [0/60000 (0%)]\tLoss: 0.025320\n",
      "Train Epoch: 37/40 [12800/60000 (21%)]\tLoss: 0.267640\n",
      "Train Epoch: 37/40 [25600/60000 (43%)]\tLoss: 0.110506\n",
      "Train Epoch: 37/40 [38400/60000 (64%)]\tLoss: 0.115973\n",
      "Train Epoch: 37/40 [51200/60000 (85%)]\tLoss: 0.050075\n",
      "\n",
      "[37] Test Loss: 0.0311, Accuracy: 99.01%\n",
      "\n",
      "Train Epoch: 38/40 [0/60000 (0%)]\tLoss: 0.086365\n",
      "Train Epoch: 38/40 [12800/60000 (21%)]\tLoss: 0.141073\n",
      "Train Epoch: 38/40 [25600/60000 (43%)]\tLoss: 0.203746\n",
      "Train Epoch: 38/40 [38400/60000 (64%)]\tLoss: 0.142008\n",
      "Train Epoch: 38/40 [51200/60000 (85%)]\tLoss: 0.048800\n",
      "\n",
      "[38] Test Loss: 0.0318, Accuracy: 99.02%\n",
      "\n",
      "Train Epoch: 39/40 [0/60000 (0%)]\tLoss: 0.196118\n",
      "Train Epoch: 39/40 [12800/60000 (21%)]\tLoss: 0.107518\n",
      "Train Epoch: 39/40 [25600/60000 (43%)]\tLoss: 0.092750\n",
      "Train Epoch: 39/40 [38400/60000 (64%)]\tLoss: 0.056288\n",
      "Train Epoch: 39/40 [51200/60000 (85%)]\tLoss: 0.113571\n",
      "\n",
      "[39] Test Loss: 0.0307, Accuracy: 99.01%\n",
      "\n",
      "Train Epoch: 40/40 [0/60000 (0%)]\tLoss: 0.032724\n",
      "Train Epoch: 40/40 [12800/60000 (21%)]\tLoss: 0.091542\n",
      "Train Epoch: 40/40 [25600/60000 (43%)]\tLoss: 0.071858\n",
      "Train Epoch: 40/40 [38400/60000 (64%)]\tLoss: 0.078717\n",
      "Train Epoch: 40/40 [51200/60000 (85%)]\tLoss: 0.066707\n",
      "\n",
      "[40] Test Loss: 0.0312, Accuracy: 98.92%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Epochs를 반복하면서 진행\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    # 모델 학습 진행\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    \n",
    "    # Epoch 반복하면서 테스트 데이터 평가\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print()\n",
    "    print(\"[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%\\n\".format(epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607ee26-720b-42c9-8c04-9d9ee1fb5c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e571a-583b-4451-99d2-b2552a7fc2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
